In order to simulate cases in which our model may use shortcuts or spurious correlations to make predictions, we created a corrupted version of our dataset. This corrupted dataset includes images with specific markers added to them. In particular, we added a _L_ marker to images of class 1 (e.g., "Pneumonia"), leaving the ones of class 0 (e.g., "Normal") unaltered. This setup allows us to investigate whether the model relies on the presence of the marker to make its predictions, rather than learning the actual features of the diseases.

What's more, it lets us test whether our explainability methods can effectively identify these spurious correlations. By analyzing the model's predictions and the explanations provided by various XAI techniques, we can assess if the model is indeed focusing on the marker rather than the relevant medical features in the images.